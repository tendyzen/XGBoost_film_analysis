# XGBoost for film profitability analysis

# 1-import libraries
# 2-data processing
# 3-data modeling
# 4-model performance
# 5-feature importance


# 1-import libraries


#!pip install shap						#make sure to install SHAP values				
import matplotlib.pyplot as plt										
import seaborn as sns										
import pandas as pd										
import xgboost as xgb										
from sklearn.model_selection import train_test_split										
from sklearn.metrics import mean_squared_error										
import shap										
										

# 2-data processing


#import csv 										
data = pd.read_csv('MLfilm1.csv')		

#drop unnecessary columns for modeling
data = data.drop(['title', 'film_id'], axis=1)	

#one-hot encode the categorical data
data_encoded = pd.get_dummies(data, columns=['rating', 'category'])		

#check data
data_encoded.head()										


# 3-data modeling


###  Start the modeling by defining y as target variable, X as everything except target
X = data_encoded.drop('gross_profit', axis=1)										
y = data_encoded['gross_profit']										

#scikit's train test split makes data partitioning easy
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)										

#define and fit the model
model = xgb.XGBRegressor()										
model.fit(X_train, y_train)										


# 4-model performance


#quantify the model with MSE 
predictions = model.predict(X_test)										
mse = mean_squared_error(y_test, predictions)										
print("Mean Squared Error:", mse)										


# 5-feature importance


#further quantify feature importance and display SHAP values
explainer = shap.Explainer(model)										
shap_values = explainer.shap_values(X_test)										
shap.summary_plot(shap_values, X_test, 										
                  feature_names=X_test.columns)										
